## **Course Project: M-L based Exercise Manner Prediction**
#### *Terence LIU, 7.2015*

<br/>

### **Background**

This is a course project of **Practical Machine Learning** in the **Data Science Specialization**. The main work of this report is to apply two useful machine learning (M-L) tecniques including **Classification trees** and **Boosting with trees** to the prediction of excecise manner. More background information is available from < http://groupware.les.inf.puc-rio.br/har>.

### **Data Loading & preprocessing**

We set the working dicrectory, load essential packages and training and testing data. Note that all predict varibles are set to be numerical type.

```{r,warning=FALSE}
library(caret)
library(rpart)
library(gbm)
library(plyr)
training <- read.csv("pml-training.csv",header=T)
testing <- read.csv("pml-testing.csv",header=T)
training <- training[,-1]
testing <- testing[,-1]  #the 1st colume X is obviously not considered as predictor.
for (i in 1:(dim(training)[2]-1)) { training[,i] <- as.numeric(training[,i])}
for (i in 1:dim(testing)[2]) { testing[,i] <- as.numeric(testing[,i])}
```

We further filter out the training varibles containing of NA to obtain more efficient and reasonable models.

```{r}
ind <- NULL
for (i in 1:(dim(training)[2])) {
ind[i] <- all(!is.na(training[,i]))
}
training <- training[,ind]
testing <- testing[,ind]
```


### **Model Construction & Prediction**

#### Algorithm selection by cross-validation

In this session, we perform the **cross-validation** by random subsampling on the training data to select the optimal algorithm. Original test set completely untouched, so when prediction algorithms are applied, the result will be an unbiased measurement of the **out of sample error** of the model. Here we set the number of random subsampling to 5 times since modeling repeatedly is too time-consuming on the available device. In each time we split original training set into sub-training and validate set, built model on sub-training set and finally average the accuracy to estimate the error.

```{r,warning=FALSE,message=FALSE}
acc.ct <- NULL
acc.gbm <- NULL
for (j in 1:5) {
set.seed(j)
inTrain <- createDataPartition(training$classe,p=0.7,list=F)
traindat <- training[inTrain,]
validdat <- training[-inTrain,]
modFit.ct <- train(classe~.,data=traindat,method="rpart")
modFit.gbm <- train(classe~.,data=traindat,method="gbm",verbose=F)
acc.ct[j] <- confusionMatrix(validdat$classe,predict(modFit.ct,newdata=validdat,na.action = na.pass))$overall[1]
acc.gbm[j] <- confusionMatrix(validdat$classe,predict(modFit.gbm,newdata=validdat,na.action = na.pass))$overall[1]
}

1-mean(acc.ct)
1-mean(acc.gbm)
```

From the error calculation of scc.ct and acc.gbm, we can get the better performing algorithm, the **Boosting with trees** of which estimated out of sample error is 0.5104673.  

#### Exercise Manner Prediction by Boosting

In this session, we perform the constructed boosting tree based model on the testing set. Firstly we construct the model with the original training set.

```{r,warning=FALSE}
modFit <- train(classe~.,data=training,method="gbm",verbose=F)
modFit
```

The constructed model have reletively high accuracy. Then we predict results of the testing set.

```{r}
predict(modFit, newdata=testing,na.action = na.pass)
```

From the results above, we obtian the predicted exercise manner types are: C B B A A E C B A A B C B A B E A E A B. 

### **Conclusion**

In this report we apply the better performing boosting with tree algrothm on exercise manner prediction. Since the estimated error are reletively high, more powerful modelling strategies should be explored in further research.


